# Data loader parameters
task_name: "long-horizon-forecasting"
train_batch_size : 64 # 1024 2048 3072 4096
val_batch_size: 256 # 1024 2048 3072 4096 
shuffle: True
num_workers: 5
pin_memory: True
seq_len : 512
scale : True 
train_ratio : 0.7
val_ratio : 0.1
test_ratio : 0.2
random_seed : 13
upsampling_pad_direction : "backward"
upsampling_type : "pad" # pad by default
downsampling_type : "interpolate"
pad_mode : "edge" # constant by default
pad_constant_values : null

# Data parameters
output_type: 'multivariate' # 'multivariate' 'univariate'

# Experiment parameters
pretraining_run_name: "fearless-planet-52" # "fearless-planet-52"
pretraining_opt_steps: null
pct_start: 0.3
max_epoch: 5
lr_scheduler_type: 'onecyclelr' 
forecast_horizon: 96
finetuning_mode: "linear-probing" # "linear-probing" "end-to-end"
dataset_names: '/TimeseriesDatasets/forecasting/autoformer/ETTh1.csv'
debug: False
init_lr: 0.00005 # 5e-5
loss_type: "mse" # MSE by default

# Model parameters
model_name: "MOMENT"
seq_len: 512
patch_len: 8
patch_stride_len: 8
transformer_backbone: 'google/flan-t5-large' # 'google/flan-t5-base' 'google/flan-t5-large'
add_positional_embedding: False
set_input_mask: True # True by default 
head_dropout: 0.1
weight_decay: 0