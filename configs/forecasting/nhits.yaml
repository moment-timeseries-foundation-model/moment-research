# Experiment parameters
run_name: Finetuning - NHits
random_seed : 13
debug: False

# Data loader parameters
task_name: "long-horizon-forecasting"
train_batch_size : 64 # 1024 2048 3072 4096
val_batch_size: 256 # 1024 2048 3072 4096 
shuffle: True
num_workers: 5
pin_memory: True
scale : True # Do not scale short horizon datasets
train_ratio : 0.7
val_ratio : 0.1
test_ratio : 0.2
random_seed : 13
upsampling_pad_direction : "backward"
upsampling_type : "pad" # pad by default
downsampling_type : "interpolate"
pad_mode : "edge" # constant by default
pad_constant_values : null

# Data parameters
output_type: 'multivariate'

# Experiment parameters
run_name: null
pct_start: 0.3
max_epoch: 10
lr_scheduler_type: 'onecyclelr' # 'linearwarmupcosinelr' 'onecyclelr'
finetuning_mode: "end-to-end"
dataset_names: '/TimeseriesDatasets/forecasting/autoformer/ETTh1.csv'
init_lr: 0.0001 # 1e-4
loss_type: "mse"
use_amp: False # Do not used mixed precision training

# Model parameters
model_name: "N-HITS"
forecast_horizon: 0
seq_len: 512
futr_exog_list: None
hist_exog_list: None
stat_exog_list: None
exclude_insample_y: False,
stack_types: !!python/tuple ["identity", "identity", "identity"]
n_blocks: !!python/tuple [1, 1, 1]
mlp_units: !!python/tuple [[512, 512], [512, 512], [512, 512]]
n_pool_kernel_size: !!python/tuple [2, 2, 1]
n_freq_downsample: !!python/tuple [4, 2, 1]
pooling_mode: "MaxPool1d"
interpolation_mode: "linear"
dropout_prob_theta: 0.0
activation: "ReLU"
valid_loss: None
max_steps: 1000
learning_rate: 0.001
num_lr_decays: 3
early_stop_patience_steps: -1
val_check_steps: 100
batch_size: 32
valid_batch_size: None
windows_batch_size: 1024
inference_windows_batch_size: -1
start_padding_enabled: False
step_size: 1
scaler_type: "identity"
random_seed: 1
num_workers_loader: 0
drop_last_loader: False
